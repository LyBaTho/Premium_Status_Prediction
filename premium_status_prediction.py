# -*- coding: utf-8 -*-
"""Premium Status Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lHIm7iceyShXIvHPwhYEt809zAcR-Xcl
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Section 1: Data Loading"""

df = pd.read_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Finance/Project 2: Premium Status/Data/insurance.csv')

df.head()

df.info()

df.rename(columns={'Income':'income',
                   'Count_3-6_months_late':'count_3_6_months_late',
                   'Count_6-12_months_late':'count_6_12_months_late',
                   'Count_more_than_12_months_late':'count_more_than_12_months_late'},inplace=True)

df.columns

df.reset_index(inplace=True)

df.drop(columns=['id','index'],inplace=True)

df.head()

"""## Section 2: Data Cleaning"""

df.isnull().any()

df.isnull().sum()

df[df.isnull().any(axis=1)]

df.dropna(inplace=True)

df.isnull().sum()

df.duplicated().any()

df.reset_index(inplace=True)

df.drop(columns='index',inplace=True)

df.head()

df.shape

df['age'] = df['age_in_days'] // 365

df.drop(columns='age_in_days',inplace=True)

df['total_late_payments'] = df['count_3_6_months_late'] + df['count_6_12_months_late'] + df['count_more_than_12_months_late']

df.info()

"""## Section 3: EDA

### Univariate Analysis

#### Categorical Variables
"""

df['sourcing_channel'] = df['sourcing_channel'].astype('category')
df['residence_area_type'] = df['residence_area_type'].astype('category')

cat_col = df.select_dtypes(include='category').columns
cat_col

def univariate_cat(df,col):
  print(df[col].value_counts())
  sns.countplot(data=df,x=col,hue=col)
  plt.xticks(rotation=60)
  plt.show()

for col in cat_col:
  print('\n* Categorical Variable:', col)
  univariate_cat(df,col)
  print()

"""**Comment:**

  * **sourcing channel:** Categories D & E have very few instances compared to others. Therefore, my solution is that I combine them into an "other" category reduces this noise and helps the model focus on more significant patterns.

#### Numerical Variables
"""

num_col = df.select_dtypes('number').columns
num_col

def univariate_num(df,col):
  print(df[col].describe())
  print('Range: ',df[col].max()-df[col].min())
  print('Var: ',df[col].var())
  print('Skewness: ',df[col].skew())
  print('Kurtosis: ',df[col].kurt())
  fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
  sns.boxplot(data=df,y=col,ax=ax1)
  sns.histplot(data=df,x=col,kde=True,ax=ax2)
  plt.show()

for col in num_col:
  print('\n* Numerical Variable:',col)
  univariate_num(df,col)
  print()

"""**Comment:**

  * **perc_premium_paid_by_cash_credit:**
    * Skewness = 0.88 (moderaly skewed, positive data)
    * Kurtosis = -0.59 (low kurtosis, light tail with no outliers)
    * Observation: The data is moderately positively skewed with a significant number of values clustered towards the lower end. This might indicate a need for transformation if this skewness affects model performance.

  * **income:**
    * Skewness = 83 (highly skewed, positive data)
    * Kurtosis = 10145 (Extremely high kurtosis, indicating a very heavy tail with many outliers)
    * Observation: The data is highly positively skewed with extreme values and numerous outliers. This strongly suggests a need for transformation or outlier treatment to normalize the data distribution.

  * **count_3-6_months_late:**
    * Skewness = 4 (highly skewed, positive data)
    * Kurtosis = 24 (very high kurtosis, heavy tail with many outliers)
    * Observation: The data is highly positively skewed with a large number of zeros and a few high values (outliers). This indicates a need for transformation or handling of zeros and outliers to improve the data distribution.

  * **count_6-12_months_late:**
    * Skewness = 10 (highly skewed, positive data)
    * Kurtosis = 185 (very high kurtosis, heavy tail with many outliers)
    * Observation: The data is extremely positively skewed with a large number of zeros and outliers. This suggests a strong need for transformation.

  * **count_more_than_12_months_late:**
    * Skewness = 7.78 (highly skewed, positive data)
    * Kurtosis = 96 (very high kurtosis, heavy tail with many outliers)
    * Observation: The data is highly positively skewed with a large number of zeros and a few outliers. Transformation and outlier treatment are recommended to handle the skewness and improve the distribution.

  * **total_late_payments:**
    * Skewness = 4.54 (highly skewed, positive data)
    * Kurtosis = 30 (very high kurtosis, heavy tail with many outliers)
    * Observation: The data is highly positively skewed with a large number of zeros and a few outliers. Transformation and outlier treatment are recommended to handle the skewness and improve the distribution.

  * **application_underwriting_score:**
    * Skewness = -2.75 (moderately skewed, negative data)
    * Kurtosis = 13.99 (very high kurtosis, heavy tail with many outliers)
    * Observation: The data is moderately negatively skewed with high kurtosis, suggesting the presence of outliers. Although the skewness is moderate, the high kurtosis indicates that some values may need to be transformed or handled to reduce the tail and improve data symmetry.

  * **no_of_premiums_paid:**
    * Skewness = 1.26 (moderately skewed, positive data)
    * Kurtosis = 3.41 (high kurtosis, heavy tail with many outliers)
    * Observation: The data is moderately positively skewed with high kurtosis, suggesting the presence of outliers. Although the skewness is moderate, the high kurtosis indicates that some values may need to be transformed or handled to reduce the tail and improve data symmetry.

### Bivariate Analysis

#### Input - Target (Cat - Num)
"""

target = 'target'

num_col = num_col.tolist()

num_col.remove(target)

num_col

cat_col

# ANOVA

import statsmodels.api as sm
from statsmodels.formula.api import ols

def bivariate_anova(df, col1, col2):
  df_sub = df[[col1,col2]]
  plt.figure(figsize=(5,6))
  sns.boxplot(data=df_sub, x=col1, y=col2)
  plt.show()
  model = ols(f'{col2} ~ C({col1})', data=df_sub).fit()
  anova_table = sm.stats.anova_lm(model, typ=2)
  print(anova_table)

col2 = 'target'
for i in range(0, len(cat_col)):
  col1 = cat_col[i]
  print('\n* Numerical Variable:', col2)
  print('* Categorical Variable:', col1)
  print('\n')
  bivariate_anova(df, col1, col2)
  print()

"""**Comment:**

  * **target and sourcing_channel:** p-value = 8.19e-28 < 0.05 => Reject H0. There is sufficient evidence to suggest that the means of the target variable differ significantly across the different categories of the sourcing_channel variable. This indicates that sourcing_channel has a significant effect on the target variable, and thus it is an important factor to consider in further analysis or modeling.
  
  * **target and residence_area_type:** p-value = 0.59 > 0.05 => Fail to reject H0. There is insufficient evidence to suggest that the means of the target variable differ significantly across the different categories of the residence_area_type variable. This suggests that residence_area_type does not have a significant effect on the target variable.

=> **Attention on residence_area_type Column**

#### Input - Target (Num - Num)
"""

import scipy
import scipy.stats as stats

def bivariate_correlation(df,col1,col2):
  print('Pearson Correlation: ')
  print(stats.pearsonr(df[col1], df[col2]))
  print('Spearman Correlation: ')
  print(stats.spearmanr(df[col1], df[col2]))
  sns.scatterplot(data=df,x=col1,y=col2)
  plt.show()

col1 = 'target'
for i in range (0, len(num_col)):
    col2 = num_col[i]
    print('\n* Numerical Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_correlation(df,col1,col2)
    print()

"""**Comment:**

  * **target and perc_premium_paid_by_cash_credit:** statistic= -0.23, p-value = 0 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Nearly no correlated)
  * **target and income:** statistic= 0.02, p-value = 2.73e-08 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Nearly no correlated)
  * **target and count_3-6_months_late:** statistic= -0.25, p-value = 0 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Lowly negatively correlated)
  * **target and count_6-12_months_late:** statistic= -0.28, p-value = 0 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Lowly negatively correlated)
  * **target and count_more_than_12_months_late:** statistic= -0.24, p-value = 0 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Lowly negatively correlated)
  * **target and total_late_payments:** statistic= -0.35, p-value = 0 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Lowly negatively correlated)
  * **target and application_underwriting_score:** statistic= 0.06, p-value = 1.12e-83 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Nearly no correlated)
  * **target and no_of_premiums_paid:** statistic= 0.01, p-value = 3.79e-07 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Nearly no correlated)
  * **target and premium:** statistic= 0.03, p-value = 6.3e-22 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Nearly no correlated)
  * **target and age:** statistic= 0.09, p-value = 2.12e-145 < 0.05 => Reject H0. There is sufficient evidence to suggest there is an association between the two variables (Nearly no correlated)

#### Input - Input (Num - Cat)
"""

for i in range(0, len(cat_col)):
  col1 = cat_col[i]
  for j in range(i+1, len(num_col)):
    col2 = num_col[j]
    print('\n* Categorical Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_anova(df, col1, col2)
    print()

"""**Comment:**

  * **sourcing_channel:** to other numerical variables, p-value is < 0.05 => Reject H0.There is sufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable
  * **residence_area_type and premium:** p-value = 1.651772e-19 < 0.05 => Reject H0. There is sufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable
  * **residence_area_type:** to other numerical variables, p-value is > 0.05 => Fail to Reject H0.There is insufficient evidence to suggest that there is a significant impact of numerical variable on the categories of the categorical variable

=> **Attention on 'sourcing_channel' Column**

#### Input - Input (Cat - Cat)
"""

from scipy.stats import chi2_contingency
from scipy.stats import chi2

def bivariate_contingency_table_chi_square(col1, col2):
  # Contingency table
  contingency_table = pd.crosstab(col1,col2)
  print(contingency_table)
  sns.barplot(data=contingency_table, ci=None)
  plt.show()

  # Chi-square test
  chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
  print('dof= ', dof)
  print('p-value= ',p_value)
  alpha = 0.05
  if p_value <= alpha:
    print('Reject H0. There is a significant association between the two variables')
  else:
    print('Fail to reject H0. There is no significant association between the two variables')

for i in range (0, len(cat_col)):
  for j in range(i+1, len(cat_col)):
    col1 = cat_col[i]
    col2 = cat_col[j]
    print('\n* Categorical Variable:', col1)
    print('* Categorical Variable:', col2)
    print('\n')
    bivariate_contingency_table_chi_square(df[col1],df[col2])
    print()

"""#### Input - Input (Num - Num)"""

for i in range (0, len(num_col)):
  for j in range(i+1, len(num_col)):
    col1 = num_col[i]
    col2 = num_col[j]
    print('\n* Numerical Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_correlation(df,col1,col2)
    print()

"""**Comment:**

  * **income and count_3_6_months_late:** statistic=-0.0008, p-value = 0.8 > 0.05 => Fail to Reject H0. There is insufficient evidence to suggest there is a significant association between the two variables (no significant correlation)

Overall, all the numerical variables have a statistically significance between them. However, the strength of correlation between them is very weak so the significance or strength of these relationships is minimal due to the low correlation coefficient.

## Section 4: Feature Selection (Training Set)

Observations About 'residence_area_type':

1.   **Input-Output:** There is insufficient evidence to suggest that the means of the target variable differ significantly across the different categories of the residence_area_type variable due to p-value > 0.05. This suggests that residence_area_type does not have a significant effect on the target variable.
2.   **Consideration:** The lack of association with the target variable suggests that principal may not be a strong predictor. However, it does not have any association with other input variables
3.   **Recommendation: Retain for Further Consideration:** Consider domain knowledge and practical significance. If residence_area_type is theoretically important or provides interpretative value, it should not be eliminated solely based on statistical tests.

Observations About 'total_late_payments':

1.   **Input-Output:** There is sufficient evidence to suggest that there is an association between total_late_payments and target column. It can be said that it does have a significant effect on the target variable.
2.   **Input-Input:** This feature has a very strong correlation with the other three features since it is derived from them.
2.   **Consideration:** The association with the target variable suggests that total_late_payments may be a strong predictor.
3.   **Recommendation:** Retain total_late_payments and remove the other three original variables to reduce noise, avoid multicollinearity, and simplify the model. This allows total_late_payments to convey the necessary information more efficiently.

Observations About other Numerical Variables

1.   **Input-Output:** There is sufficient evidence to suggest there is an association between the two variables
2.   **Input-Input:** Even though they are statistically significant in association with one another, the strength of correlation among them is very weak. Therefore, we can generally assume they are not dependent on one another
3.   **Consideration:** The association with the target variable suggests that these input numerical variables may  be a strong predictor. Besides, it does not have any association with other input variables
4.   **Recommendation: Keep them for the modeling** The distribution of those numerical variables are highly skewed so they need to be fine-tuned before building model
"""

df['sourcing_channel'] = df['sourcing_channel'].replace(['D','E'],'Other')

df[['sourcing_channel','target']].groupby('sourcing_channel',as_index=False).count()

df.drop(columns=['count_3_6_months_late','count_6_12_months_late','count_more_than_12_months_late'],inplace=True)

df.head()

df.columns

"""## Section 5: Dataset Splitting"""

from sklearn.model_selection import train_test_split

x = df[['perc_premium_paid_by_cash_credit', 'income',
       'application_underwriting_score', 'no_of_premiums_paid',
       'sourcing_channel', 'residence_area_type', 'premium', 'age','total_late_payments']]

y = df[['target']]

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=40)

test_set = pd.concat([x_test,y_test],axis=1)

test_set.head()

test_set.to_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Finance/Project 2: Premium Status/Data/insurance_test_set.csv',index=False)

train_set = pd.concat([x_train,y_train],axis=1)

train_set.head()

"""## Section 6: Data Preprocessing (Training Set)"""

from sklearn import preprocessing

cat_col

train_set['residence_area_type'].unique()

train_set['residence_area_type_encoder'] = train_set['residence_area_type'].map(lambda x: 1 if x == 'Urban' else 0)
train_set[['residence_area_type','residence_area_type_encoder']]

train_set['residence_area_type_encoder'] = train_set['residence_area_type_encoder'].astype('int')

train_set.drop(columns='residence_area_type',inplace=True)

train_set['sourcing_channel'].unique()

sourcing_channel_map = {'A':0,'B':1,'C':2,'Other':3}
train_set['sourcing_channel_mapped'] = train_set['sourcing_channel'].map(sourcing_channel_map)
sourcing_channel_label_encoder = preprocessing.LabelEncoder()
sourcing_channel_label_encoder.fit(train_set['sourcing_channel_mapped'])
train_set['sourcing_channel_encoder'] = sourcing_channel_label_encoder.transform(train_set['sourcing_channel_mapped'])

train_set[['sourcing_channel','sourcing_channel_encoder']].head()

train_set.drop(columns=['sourcing_channel_mapped','sourcing_channel'],inplace=True)

num_col

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
sns.boxplot(data=train_set,y='perc_premium_paid_by_cash_credit',ax=ax1)
sns.histplot(data=train_set,x='perc_premium_paid_by_cash_credit',ax=ax2)
plt.show()

train_set['perc_premium_paid_by_cash_credit_binned'] = pd.cut(train_set['perc_premium_paid_by_cash_credit'],4)
train_set[['perc_premium_paid_by_cash_credit_binned','target']].groupby('perc_premium_paid_by_cash_credit_binned',as_index=False).count()

# Create a new column for binned values
train_set['perc_premium_paid_by_cash_credit_binned_num'] = np.nan

# Assign numerical labels based on the bins
train_set.loc[train_set['perc_premium_paid_by_cash_credit'] <= 0.25, 'perc_premium_paid_by_cash_credit_binned_num'] = 0
train_set.loc[(train_set['perc_premium_paid_by_cash_credit'] > 0.25) & (train_set['perc_premium_paid_by_cash_credit'] <= 0.5), 'perc_premium_paid_by_cash_credit_binned_num'] = 1
train_set.loc[(train_set['perc_premium_paid_by_cash_credit'] > 0.5) & (train_set['perc_premium_paid_by_cash_credit'] <= 0.75), 'perc_premium_paid_by_cash_credit_binned_num'] = 2
train_set.loc[train_set['perc_premium_paid_by_cash_credit'] > 0.75, 'perc_premium_paid_by_cash_credit_binned_num'] = 3

# Verify the result by counting the number of targets in each bin
result = train_set[['perc_premium_paid_by_cash_credit_binned_num', 'target']].groupby('perc_premium_paid_by_cash_credit_binned_num', as_index=False).count()

result

train_set.drop(columns=['perc_premium_paid_by_cash_credit_binned','perc_premium_paid_by_cash_credit'],inplace=True)

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
sns.boxplot(data=train_set,y='income',ax=ax1)
sns.histplot(data=train_set,x='income',ax=ax2)
plt.show()

train_set['income'].skew()

train_set['income'].kurt()

# Calculate Q1, Q3, IQR
Q1 = train_set['income'].quantile(0.1)
Q3 = train_set['income'].quantile(0.9)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
train_set = train_set[(train_set['income'] >= lower_bound) & (train_set['income'] <= upper_bound)]

train_set['income'].skew()

train_set['income'].kurt()

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
sns.boxplot(data=train_set,y='income',ax=ax1)
sns.histplot(data=train_set,x='income',ax=ax2)
plt.show()

train_set['income_binned'] = pd.cut(train_set['income'],5)

train_set[['income_binned','target']].groupby('income_binned',as_index=False).count()

train_set['income_binned_num'] = np.NaN

# Assign numerical labels based on the bins
train_set.loc[train_set['income'] <= 173090.0, 'income_binned_num'] = 0
train_set.loc[(train_set['income'] > 173090.0) & (train_set['income'] <= 322150.0), 'income_binned_num'] = 1
train_set.loc[(train_set['income'] > 322150.0) & (train_set['income'] <= 471210.0), 'income_binned_num'] = 2
train_set.loc[(train_set['income'] > 471210.0) & (train_set['income'] <= 620270.0), 'income_binned_num'] = 3
train_set.loc[train_set['income'] > 620270.0, 'income_binned_num'] = 4

train_set[['income_binned_num', 'target']].groupby('income_binned_num', as_index=False).count()

train_set.drop(columns=['income_binned','income'],inplace=True)

train_set['application_underwriting_score'] = train_set['application_underwriting_score'] / 100

train_set.head()

train_set['no_of_premiums_paid_binned'] = pd.cut(train_set['no_of_premiums_paid'],4)

train_set[['no_of_premiums_paid_binned','target']].groupby('no_of_premiums_paid_binned',as_index=False).count()

train_set['no_of_premiums_paid_binned_num'] = np.nan

train_set.loc[train_set['no_of_premiums_paid'] <= 16, 'no_of_premiums_paid_binned_num'] = 0
train_set.loc[(train_set['no_of_premiums_paid'] > 16) & (train_set['no_of_premiums_paid'] <= 30), 'no_of_premiums_paid_binned_num'] = 1
train_set.loc[(train_set['no_of_premiums_paid'] > 30) & (train_set['no_of_premiums_paid'] <= 44), 'no_of_premiums_paid_binned_num'] = 2
train_set.loc[train_set['no_of_premiums_paid'] > 44, 'no_of_premiums_paid_binned_num'] = 3

train_set[['no_of_premiums_paid_binned_num','target']].groupby('no_of_premiums_paid_binned_num',as_index=False).count()

train_set.drop(columns=['no_of_premiums_paid','no_of_premiums_paid_binned'],inplace=True)

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
sns.boxplot(data=train_set,y='premium',ax=ax1)
sns.histplot(data=train_set,x='premium',ax=ax2)
plt.show()

train_set['premium'].skew()

train_set['premium'].kurt()

# Calculate Q1, Q3, IQR
Q1 = train_set['premium'].quantile(0.15)
Q3 = train_set['premium'].quantile(0.85)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
train_set = train_set[(train_set['premium'] >= lower_bound) & (train_set['premium'] <= upper_bound)]

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
sns.boxplot(data=train_set,y='premium',ax=ax1)
sns.histplot(data=train_set,x='premium',ax=ax2)
plt.show()

train_set['premium'].skew()

train_set['premium'].kurt()

train_set['premium_binned'] = pd.cut(train_set['premium'],5)

train_set[['premium_binned','target']].groupby('premium_binned',as_index=False).count()

train_set['premium_binned_num'] = np.NaN

# Assign numerical labels based on the bins
train_set.loc[train_set['premium'] <= 8760.0, 'premium_binned_num'] = 0
train_set.loc[(train_set['premium'] > 8760.0) & (train_set['premium'] <= 16320.0), 'premium_binned_num'] = 1
train_set.loc[(train_set['premium'] > 16320.0) & (train_set['premium'] <= 23880.0), 'premium_binned_num'] = 2
train_set.loc[(train_set['premium'] > 23880.0) & (train_set['premium'] <= 31440.0), 'premium_binned_num'] = 3
train_set.loc[train_set['premium'] > 31440.0, 'premium_binned_num'] = 4

train_set[['premium_binned_num','target']].groupby('premium_binned_num',as_index=False).count()

train_set.drop(columns=['premium','premium_binned'],inplace=True)

train_set['age_binned'] = pd.cut(train_set['age'],4)

train_set[['age_binned','target']].groupby('age_binned',as_index=False).count()

train_set['age_binned_num'] = np.NaN

# Assign numerical labels based on the bins
train_set.loc[train_set['age'] <= 41.25, 'age_binned_num'] = 0
train_set.loc[(train_set['age'] > 41.25) & (train_set['age'] <= 61.5), 'age_binned_num'] = 1
train_set.loc[(train_set['age'] > 61.5) & (train_set['age'] <= 81.75), 'age_binned_num'] = 2
train_set.loc[train_set['age'] > 81.75, 'age_binned_num'] = 3

train_set.drop(columns=['age','age_binned'],inplace=True)

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
sns.boxplot(data=train_set,y='total_late_payments',ax=ax1)
sns.histplot(data=train_set,x='total_late_payments',ax=ax2)
plt.show()

train_set['total_late_payments'].skew()

train_set['total_late_payments'].kurt()

# Calculate Q1, Q3, IQR
Q1 = train_set['total_late_payments'].quantile(0)
Q3 = train_set['total_late_payments'].quantile(0.9)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
train_set = train_set[(train_set['total_late_payments'] >= lower_bound) & (train_set['total_late_payments'] <= upper_bound)]

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
sns.boxplot(data=train_set,y='total_late_payments',ax=ax1)
sns.histplot(data=train_set,x='total_late_payments',ax=ax2)
plt.show()

train_set['total_late_payments'].skew()

train_set['total_late_payments'].kurt()

train_set['total_late_payments_binned'] = pd.cut(train_set['total_late_payments'],3)

train_set[['total_late_payments_binned','target']].groupby('total_late_payments_binned',as_index=False).count()

train_set['total_late_payments_binned_num'] = np.NaN

# Assign numerical labels based on the bins
train_set.loc[train_set['total_late_payments'] <= 0.667, 'total_late_payments_binned_num'] = 0
train_set.loc[(train_set['total_late_payments'] > 0.667) & (train_set['total_late_payments'] <= 1.333), 'total_late_payments_binned_num'] = 1
train_set.loc[train_set['total_late_payments'] > 1.333, 'total_late_payments_binned_num'] = 2

train_set[['total_late_payments_binned_num','target']].groupby('total_late_payments_binned_num',as_index=False).count()

train_set.drop(columns=['total_late_payments','total_late_payments_binned'],inplace=True)

train_set.head()

train_set.info()

train_set.columns

"""## Section 7: Dataset Splitting for Model Training (Training Set)"""

x = train_set[['application_underwriting_score',
       'residence_area_type_encoder', 'sourcing_channel_encoder',
       'perc_premium_paid_by_cash_credit_binned_num', 'income_binned_num',
       'no_of_premiums_paid_binned_num', 'premium_binned_num',
       'age_binned_num', 'total_late_payments_binned_num']]
y = train_set[['target']]

x_train, x_test, y_train, y_test =  train_test_split(x,y,test_size=0.2,random_state=40)

"""## Section 8: Prediction (Training Set)"""

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(x_train,y_train)

y_pred = logreg.predict(x_test)

accuracy_logreg = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_logreg)

"""### KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(x_train,y_train)

y_pred = knn.predict(x_test)

accuracy_knn = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_knn)

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

decision_tree = DecisionTreeClassifier()

decision_tree.fit(x_train,y_train)

y_pred = decision_tree.predict(x_test)

accuracy_decision_tree = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_decision_tree)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier()

random_forest.fit(x_train,y_train)

y_pred = random_forest.predict(x_test)

accuracy_random_forest = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_random_forest)

"""### Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

naive_bayes = GaussianNB()

naive_bayes.fit(x_train,y_train)

y_pred = naive_bayes.predict(x_test)

accuracy_nb = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_nb)

pred_values = pd.DataFrame(
    {'Algorithms':['LogisticRegression','KNN','DecisionTree','RandomForest','Naive Bayes'],
    'Accuracy Score':[accuracy_logreg,accuracy_knn,accuracy_decision_tree,accuracy_random_forest,accuracy_nb]}
)

pred_values

"""**Comment:**

  * Since Logistic Regression, and KNN have the same accuracy score, we will pick the Logistic Regression for simplicity
"""

# K-fold cross-validation

kf = KFold(n_splits=5, shuffle=True, random_state=40)

# Cross-validation scores Logistic Regression
cv_scores_logreg = cross_val_score(logreg, x, y, cv=kf, scoring='accuracy')

print('Cross-validation scores: ', cv_scores_logreg)
print('Average cross-validation accuracy: ', cv_scores_logreg.mean())

# Confusion Matrix

print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

"""*   True Negatives (TN): 130
*   False Positives (FP): 399
*   False Negatives (FN): 517
*   True Positives (TP): 10463
"""

# Classification report

print('Classification Report:')
print(classification_report(y_test, y_pred))

"""## Section 9: Data Preprocessing (Test Set)"""

test_set = pd.read_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Finance/Project 2: Premium Status/Data/insurance_test_set.csv')
test_set.head()

test_set['residence_area_type_encoder'] = test_set['residence_area_type'].map(lambda x: 1 if x == 'Urban' else 0)
test_set[['residence_area_type','residence_area_type_encoder']]

test_set['residence_area_type_encoder'] = test_set['residence_area_type_encoder'].astype('int')

test_set.drop(columns='residence_area_type',inplace=True)

sourcing_channel_map = {'A':0,'B':1,'C':2,'Other':3}
test_set['sourcing_channel_mapped'] = test_set['sourcing_channel'].map(sourcing_channel_map)
test_set['sourcing_channel_encoder'] = sourcing_channel_label_encoder.transform(test_set['sourcing_channel_mapped'])

test_set.drop(columns=['sourcing_channel_mapped','sourcing_channel'],inplace=True)

test_set['perc_premium_paid_by_cash_credit_binned'] = pd.cut(test_set['perc_premium_paid_by_cash_credit'],4)
test_set[['perc_premium_paid_by_cash_credit_binned','target']].groupby('perc_premium_paid_by_cash_credit_binned',as_index=False).count()

# Create a new column for binned values
test_set['perc_premium_paid_by_cash_credit_binned_num'] = np.nan

# Assign numerical labels based on the bins
test_set.loc[test_set['perc_premium_paid_by_cash_credit'] <= 0.25, 'perc_premium_paid_by_cash_credit_binned_num'] = 0
test_set.loc[(test_set['perc_premium_paid_by_cash_credit'] > 0.25) & (test_set['perc_premium_paid_by_cash_credit'] <= 0.5), 'perc_premium_paid_by_cash_credit_binned_num'] = 1
test_set.loc[(test_set['perc_premium_paid_by_cash_credit'] > 0.5) & (test_set['perc_premium_paid_by_cash_credit'] <= 0.75), 'perc_premium_paid_by_cash_credit_binned_num'] = 2
test_set.loc[test_set['perc_premium_paid_by_cash_credit'] > 0.75, 'perc_premium_paid_by_cash_credit_binned_num'] = 3

test_set[['perc_premium_paid_by_cash_credit_binned_num', 'target']].groupby('perc_premium_paid_by_cash_credit_binned_num', as_index=False).count()

test_set.drop(columns=['perc_premium_paid_by_cash_credit_binned','perc_premium_paid_by_cash_credit'],inplace=True)

# Calculate Q1, Q3, IQR
Q1 = test_set['income'].quantile(0.1)
Q3 = test_set['income'].quantile(0.9)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
test_set = test_set[(test_set['income'] >= lower_bound) & (test_set['income'] <= upper_bound)]

test_set['income_binned'] = pd.cut(test_set['income'],5)

test_set['income_binned_num'] = np.NaN

# Assign numerical labels based on the bins
test_set.loc[test_set['income'] <= 173090.0, 'income_binned_num'] = 0
test_set.loc[(test_set['income'] > 173090.0) & (test_set['income'] <= 322150.0), 'income_binned_num'] = 1
test_set.loc[(test_set['income'] > 322150.0) & (test_set['income'] <= 471210.0), 'income_binned_num'] = 2
test_set.loc[(test_set['income'] > 471210.0) & (test_set['income'] <= 620270.0), 'income_binned_num'] = 3
test_set.loc[test_set['income'] > 620270.0, 'income_binned_num'] = 4

test_set.drop(columns=['income_binned','income'],inplace=True)

test_set['application_underwriting_score'] = test_set['application_underwriting_score'] / 100

test_set['no_of_premiums_paid_binned'] = pd.cut(test_set['no_of_premiums_paid'],4)

test_set['no_of_premiums_paid_binned_num'] = np.nan

test_set.loc[test_set['no_of_premiums_paid'] <= 16, 'no_of_premiums_paid_binned_num'] = 0
test_set.loc[(test_set['no_of_premiums_paid'] > 16) & (test_set['no_of_premiums_paid'] <= 30), 'no_of_premiums_paid_binned_num'] = 1
test_set.loc[(test_set['no_of_premiums_paid'] > 30) & (test_set['no_of_premiums_paid'] <= 44), 'no_of_premiums_paid_binned_num'] = 2
test_set.loc[test_set['no_of_premiums_paid'] > 44, 'no_of_premiums_paid_binned_num'] = 3

test_set.drop(columns=['no_of_premiums_paid','no_of_premiums_paid_binned'],inplace=True)

# Calculate Q1, Q3, IQR
Q1 = test_set['premium'].quantile(0.15)
Q3 = test_set['premium'].quantile(0.85)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
test_set = test_set[(test_set['premium'] >= lower_bound) & (test_set['premium'] <= upper_bound)]

test_set['premium_binned'] = pd.cut(test_set['premium'],5)

test_set['premium_binned_num'] = np.NaN

# Assign numerical labels based on the bins
test_set.loc[test_set['premium'] <= 8760.0, 'premium_binned_num'] = 0
test_set.loc[(test_set['premium'] > 8760.0) & (test_set['premium'] <= 16320.0), 'premium_binned_num'] = 1
test_set.loc[(test_set['premium'] > 16320.0) & (test_set['premium'] <= 23880.0), 'premium_binned_num'] = 2
test_set.loc[(test_set['premium'] > 23880.0) & (test_set['premium'] <= 31440.0), 'premium_binned_num'] = 3
test_set.loc[test_set['premium'] > 31440.0, 'premium_binned_num'] = 4

test_set.drop(columns=['premium','premium_binned'],inplace=True)

test_set['age_binned'] = pd.cut(test_set['age'],4)

test_set['age_binned_num'] = np.NaN

# Assign numerical labels based on the bins
test_set.loc[test_set['age'] <= 41.25, 'age_binned_num'] = 0
test_set.loc[(test_set['age'] > 41.25) & (test_set['age'] <= 61.5), 'age_binned_num'] = 1
test_set.loc[(test_set['age'] > 61.5) & (test_set['age'] <= 81.75), 'age_binned_num'] = 2
test_set.loc[test_set['age'] > 81.75, 'age_binned_num'] = 3

test_set.drop(columns=['age','age_binned'],inplace=True)

# Calculate Q1, Q3, IQR
Q1 = test_set['total_late_payments'].quantile(0)
Q3 = test_set['total_late_payments'].quantile(0.9)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
test_set = test_set[(test_set['total_late_payments'] >= lower_bound) & (test_set['total_late_payments'] <= upper_bound)]

test_set['total_late_payments_binned'] = pd.cut(test_set['total_late_payments'],3)

test_set['total_late_payments_binned_num'] = np.nan

# Assign numerical labels based on the bins
test_set.loc[test_set['total_late_payments'] <= 0.667, 'total_late_payments_binned_num'] = 0
test_set.loc[(test_set['total_late_payments'] > 0.667) & (test_set['total_late_payments'] <= 1.333), 'total_late_payments_binned_num'] = 1
test_set.loc[test_set['total_late_payments'] > 1.333, 'total_late_payments_binned_num'] = 2

test_set.drop(columns=['total_late_payments','total_late_payments_binned'],inplace=True)

test_set.info()

test_set.columns

test_set.head()

"""## Section 10: Prediction (Test Set)"""

# Make predictions on the test set
y_pred_test = logreg.predict(test_set[['application_underwriting_score',
       'residence_area_type_encoder', 'sourcing_channel_encoder',
       'perc_premium_paid_by_cash_credit_binned_num', 'income_binned_num',
       'no_of_premiums_paid_binned_num', 'premium_binned_num',
       'age_binned_num', 'total_late_payments_binned_num']])

# Accuracy score

test_accuracy = accuracy_score(test_set['target'], y_pred_test)
print('Test Set Accuracy: ', test_accuracy)

# Confusion matrix

test_confusion_matrix = confusion_matrix(test_set['target'], y_pred_test)
print('Confusion Matrix:')
print(test_confusion_matrix)

# Classification report

test_classification_report = classification_report(test_set['target'], y_pred_test)
print('Classification Report:')
print(test_classification_report)